{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6069e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "import torchvision \n",
    "from torchvision import datasets,models,transforms \n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import os \n",
    "import time \n",
    "import copy \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a381338",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_gray = 0.1307\n",
    "stddev_gray = 0.3081\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((mean_gray,),(stddev_gray,))\n",
    "])\n",
    "\n",
    "#Load MNIST data \n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform = data_transform,\n",
    "    download=False\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform = data_transform,\n",
    "    download=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56dbb73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape is 60000\n",
      "Test data shape is 10000\n"
     ]
    }
   ],
   "source": [
    "print('Train data shape is {}'.format(len(train_dataset)))\n",
    "print('Test data shape is {}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7c86cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_load = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "test_load = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4a5d2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x,y in train_load:\n",
    "    print(x.shape,y.shape)\n",
    "    break \n",
    "    \n",
    "num=4\n",
    "img = x[:num]\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(num):\n",
    "    plt.subplot(1,num+1,i+1)\n",
    "    plt.imshow(to_pil_image(mean_gray*img[i]+stddev_gray),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0878bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Teacher Model \n",
    "class Teacher(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Teacher,self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28,1200)\n",
    "        self.bn1 = nn.BatchNorm1d(1200)\n",
    "        self.fc2 = nn.Linear(1200,1200)\n",
    "        self.bn2 = nn.BatchNorm1d(1200)\n",
    "        self.fc3 = nn.Linear(1200,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.dropout(x,p=0.8)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.dropout(x,p=0.8)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "241bb93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.randn(16,1,28,28).to(device)\n",
    "teacher = Teacher().to(device)\n",
    "output = teacher(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba3f1ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.normal_(model.weight.data,0.0,0.02)\n",
    "        nn.init.constant_(model.bias.data,0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(model.weight.data,1.0,0.02)\n",
    "        nn.init.constant_(model.bias.data,0)\n",
    "        \n",
    "    teacher.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "544e38c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train teacher model \n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(teacher.parameters())\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "lr_scheduler = ReduceLROnPlateau(opt,mode='min',factor=0.1,patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9545ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(opt):\n",
    "    for param in opt.param_groups:\n",
    "        return param['lr']\n",
    "\n",
    "## calculate the metric per mini-batch\n",
    "def metric_batch(output,target):\n",
    "    pred = output.argmax(1,keepdim=True)\n",
    "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects \n",
    "\n",
    "# calculate the loss per mini-batch\n",
    "def loss_batch(loss_func,output,target,opt=None):\n",
    "    loss_b = loss_func(output,target)\n",
    "    metric_b = metric_batch(output,target)\n",
    "    \n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss_b.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    return loss_b.item(),metric_b \n",
    "\n",
    "# calculate the loss per epochs\n",
    "def loss_epoch(model,loss_func,dataset_dl,sanity_check=False,opt=None):\n",
    "    running_loss = 0.0 \n",
    "    running_metric = 0.0 \n",
    "    len_data = len(dataset_dl.dataset)\n",
    "    \n",
    "    for xb,yb in dataset_dl:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        output = model(xb)\n",
    "        \n",
    "        loss_b,metric_b = loss_batch(loss_func,output,yb,opt)\n",
    "        running_loss += loss_b \n",
    "        \n",
    "        if metric_b is not None:\n",
    "            running_metric += metric_b \n",
    "            \n",
    "        if sanity_check is True:\n",
    "            break \n",
    "            \n",
    "    loss = running_loss / len_data \n",
    "    metric = running_metric / len_data \n",
    "    return loss,metric \n",
    "\n",
    "# function to start training\n",
    "def train_val(model,params):\n",
    "    num_epochs = params['num_epochs']\n",
    "    loss_func = params['loss_func']\n",
    "    opt = params['optimizer']\n",
    "    train_dl = params['train_dl']\n",
    "    val_dl = params['val_dl']\n",
    "    sanity_check = params['sanity_check']\n",
    "    lr_scheduler = params['lr_scheduler']\n",
    "    path2weights = params['path2weights']\n",
    "    \n",
    "    loss_history = {'train':[], 'val':[]}\n",
    "    metric_history = {'train':[], 'val':[]}\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    start_time = time.time() \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr= {}'.format(epoch, num_epochs-1, current_lr))\n",
    "        \n",
    "        model.train() \n",
    "        train_loss,train_metric = loss_epoch(model,loss_func,train_dl,sanity_check,opt)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        metric_history['train'].append(train_metric)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
    "        loss_history['val'].append(val_loss)\n",
    "        metric_history['val'].append(val_metric)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print('Copied best model weights!')\n",
    "\n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            print('Loading best model weights!')\n",
    "            model.load_state_dict(best_model_wts)\n",
    "\n",
    "        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n",
    "        print('-'*10)\n",
    "        \n",
    "        model.load_state_dict(best_model_wts)\n",
    "        return model,loss_history,metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fffca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyper parameters\n",
    "params_train = {\n",
    "    'num_epochs':30,\n",
    "    'optimizer':opt,\n",
    "    'loss_func':loss_func,\n",
    "    'train_dl':train_load,\n",
    "    'val_dl':test_load,\n",
    "    'sanity_check':False,\n",
    "    'lr_scheduler':lr_scheduler,\n",
    "    'path2weights':'./models/teacher_weights.pt',\n",
    "}\n",
    "\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSerror:\n",
    "        print('Error')\n",
    "createFolder('./data')\n",
    "\n",
    "createFolder('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb60968c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29, current lr= 0.001\n"
     ]
    }
   ],
   "source": [
    "teacher, loss_hist, metric_hist = train_val(teacher, params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479648bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e0d71ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self,dropout=0.5):\n",
    "        super(LinearNet,self).__init__()\n",
    "        self.linear1 = nn.Linear(784,1200,bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.linear2 = nn.Linear(1200,1200,bias=False)\n",
    "        self.linear3 = nn.Linear(1200,10,bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x.view(x.size(0),-1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "18c2b7a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-aa9f0119d97b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'model.pth.tar'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mbig_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mbig_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_state_dict'"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "big_model = LinearNet().to(device)\n",
    "\n",
    "load_path = './teacher_linear_model/'\n",
    "if torch.cuda.is_available():\n",
    "    checkpoint = torch.load(load_path + 'model.pth.tar')\n",
    "else:\n",
    "    checkpoint = torch.load(load_path + 'model.pth.tar',  map_location=torch.device('cpu'))\n",
    "    \n",
    "big_model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "big_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8e4716e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(83)]\n",
      "[tensor(83), tensor(85)]\n",
      "[tensor(83), tensor(85), tensor(84)]\n",
      "[tensor(83), tensor(85), tensor(84), tensor(84)]\n",
      "[tensor(83), tensor(85), tensor(84), tensor(84), tensor(83)]\n"
     ]
    }
   ],
   "source": [
    "#train \n",
    "num_epochs = 5 \n",
    "batch_size = 32 \n",
    "\n",
    "train_loss = []\n",
    "train_accuracy = [] \n",
    "\n",
    "model = LinearNet()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()        \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    iterations = 0 \n",
    "    iter_loss = 0.0\n",
    "    for i,(images,labels) in enumerate(train_load):\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        iter_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _,predict = torch.max(outputs,dim=1)\n",
    "        correct += (predict == labels).sum() \n",
    "        iterations += 1 \n",
    "    train_loss.append(iter_loss / iterations)\n",
    "    train_accuracy.append((100 * correct // len(train_dataset)))\n",
    "    print(train_accuracy)\n",
    "    torch.save(model.state_dict(), 'model.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4147aaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearNet(\n",
       "  (linear1): Linear(in_features=784, out_features=1200, bias=False)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear2): Linear(in_features=1200, out_features=1200, bias=False)\n",
       "  (linear3): Linear(in_features=1200, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_path = './teacher_linear_model/'\n",
    "load_path = load_path + 'model.pth.tar'\n",
    "model.load_state_dict(torch.load(load_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "eaa8d22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(90)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    for i,(images,labels) in enumerate(test_load):\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        loss += loss.item()\n",
    "        _,predict = torch.max(outputs,dim=1)\n",
    "        correct += (predict == labels).sum()\n",
    "        iterations += 1\n",
    "    \n",
    "test_loss.append(loss/iterations)\n",
    "test_accuracy.append(100 * correct //len(test_dataset))\n",
    "\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1b36e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallLinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallLinearNet,self).__init__()\n",
    "        self.linear1 = nn.Linear(784,50,bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear2 = nn.Linear(50,10,bias=False)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = x.view(x.size(0),-1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9b8b25ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(83), tensor(85), tensor(84), tensor(84), tensor(83), tensor(97), 97]\n"
     ]
    }
   ],
   "source": [
    "small_train_loss = []\n",
    "small_train_accuracy = []\n",
    "\n",
    "model2 = SmallLinearNet()\n",
    "optimizer = torch.optim.Adam(model2.parameters(),lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    iterrations = 0\n",
    "    iter_loss = 0\n",
    "    for i,(images,labels) in enumerate(train_load):\n",
    "        outputs = model2(images)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        iter_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _,predict = torch.max(outputs,dim=1)\n",
    "        correct += (predict == labels).sum().item()\n",
    "        iterations += 1 \n",
    "        \n",
    "train_loss.append(iter_loss / iterations)\n",
    "train_accuracy.append((100 * correct // len(train_dataset)))\n",
    "print(train_accuracy)\n",
    "torch.save(model.state_dict(), 'model.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "50b2cc5f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(90),\n",
       " tensor(3089),\n",
       " tensor(3089),\n",
       " tensor(96),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(7),\n",
       " tensor(7),\n",
       " tensor(7),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(11),\n",
       " tensor(11),\n",
       " tensor(11),\n",
       " tensor(11),\n",
       " tensor(12),\n",
       " tensor(12),\n",
       " tensor(12),\n",
       " tensor(13),\n",
       " tensor(13),\n",
       " tensor(13),\n",
       " tensor(14),\n",
       " tensor(14),\n",
       " tensor(14),\n",
       " tensor(14),\n",
       " tensor(15),\n",
       " tensor(15),\n",
       " tensor(15),\n",
       " tensor(16),\n",
       " tensor(16),\n",
       " tensor(16),\n",
       " tensor(17),\n",
       " tensor(17),\n",
       " tensor(17),\n",
       " tensor(18),\n",
       " tensor(18),\n",
       " tensor(18),\n",
       " tensor(18),\n",
       " tensor(19),\n",
       " tensor(19),\n",
       " tensor(19),\n",
       " tensor(20),\n",
       " tensor(20),\n",
       " tensor(20),\n",
       " tensor(21),\n",
       " tensor(21),\n",
       " tensor(21),\n",
       " tensor(21),\n",
       " tensor(22),\n",
       " tensor(22),\n",
       " tensor(22),\n",
       " tensor(23),\n",
       " tensor(23),\n",
       " tensor(23),\n",
       " tensor(24),\n",
       " tensor(24),\n",
       " tensor(24),\n",
       " tensor(25),\n",
       " tensor(25),\n",
       " tensor(25),\n",
       " tensor(25),\n",
       " tensor(26),\n",
       " tensor(26),\n",
       " tensor(26),\n",
       " tensor(27),\n",
       " tensor(27),\n",
       " tensor(27),\n",
       " tensor(28),\n",
       " tensor(28),\n",
       " tensor(28),\n",
       " tensor(28),\n",
       " tensor(29),\n",
       " tensor(29),\n",
       " tensor(29),\n",
       " tensor(30),\n",
       " tensor(30),\n",
       " tensor(30),\n",
       " tensor(31),\n",
       " tensor(31),\n",
       " tensor(31),\n",
       " tensor(32),\n",
       " tensor(32),\n",
       " tensor(32),\n",
       " tensor(33),\n",
       " tensor(33),\n",
       " tensor(33),\n",
       " tensor(33),\n",
       " tensor(34),\n",
       " tensor(34),\n",
       " tensor(34),\n",
       " tensor(35),\n",
       " tensor(35),\n",
       " tensor(35),\n",
       " tensor(36),\n",
       " tensor(36),\n",
       " tensor(36),\n",
       " tensor(37),\n",
       " tensor(37),\n",
       " tensor(37),\n",
       " tensor(37),\n",
       " tensor(38),\n",
       " tensor(38),\n",
       " tensor(38),\n",
       " tensor(39),\n",
       " tensor(39),\n",
       " tensor(39),\n",
       " tensor(40),\n",
       " tensor(40),\n",
       " tensor(40),\n",
       " tensor(40),\n",
       " tensor(41),\n",
       " tensor(41),\n",
       " tensor(41),\n",
       " tensor(42),\n",
       " tensor(42),\n",
       " tensor(42),\n",
       " tensor(43),\n",
       " tensor(43),\n",
       " tensor(43),\n",
       " tensor(44),\n",
       " tensor(44),\n",
       " tensor(44),\n",
       " tensor(44),\n",
       " tensor(45),\n",
       " tensor(45),\n",
       " tensor(45),\n",
       " tensor(46),\n",
       " tensor(46),\n",
       " tensor(46),\n",
       " tensor(47),\n",
       " tensor(47),\n",
       " tensor(47),\n",
       " tensor(47),\n",
       " tensor(48),\n",
       " tensor(48),\n",
       " tensor(48),\n",
       " tensor(49),\n",
       " tensor(49),\n",
       " tensor(49),\n",
       " tensor(50),\n",
       " tensor(50),\n",
       " tensor(50),\n",
       " tensor(51),\n",
       " tensor(51),\n",
       " tensor(51),\n",
       " tensor(52),\n",
       " tensor(52),\n",
       " tensor(52),\n",
       " tensor(53),\n",
       " tensor(53),\n",
       " tensor(53),\n",
       " tensor(53),\n",
       " tensor(54),\n",
       " tensor(54),\n",
       " tensor(54),\n",
       " tensor(55),\n",
       " tensor(55),\n",
       " tensor(55),\n",
       " tensor(56),\n",
       " tensor(56),\n",
       " tensor(56),\n",
       " tensor(57),\n",
       " tensor(57),\n",
       " tensor(57),\n",
       " tensor(57),\n",
       " tensor(58),\n",
       " tensor(58),\n",
       " tensor(58),\n",
       " tensor(59),\n",
       " tensor(59),\n",
       " tensor(59),\n",
       " tensor(60),\n",
       " tensor(60),\n",
       " tensor(60),\n",
       " tensor(61),\n",
       " tensor(61),\n",
       " tensor(61),\n",
       " tensor(62),\n",
       " tensor(62),\n",
       " tensor(62),\n",
       " tensor(63),\n",
       " tensor(63),\n",
       " tensor(63),\n",
       " tensor(63),\n",
       " tensor(64),\n",
       " tensor(64),\n",
       " tensor(64),\n",
       " tensor(65),\n",
       " tensor(65),\n",
       " tensor(65),\n",
       " tensor(66),\n",
       " tensor(66),\n",
       " tensor(66),\n",
       " tensor(67),\n",
       " tensor(67),\n",
       " tensor(67),\n",
       " tensor(68),\n",
       " tensor(68),\n",
       " tensor(68),\n",
       " tensor(68),\n",
       " tensor(69),\n",
       " tensor(69),\n",
       " tensor(69),\n",
       " tensor(70),\n",
       " tensor(70),\n",
       " tensor(70),\n",
       " tensor(71),\n",
       " tensor(71),\n",
       " tensor(71),\n",
       " tensor(72),\n",
       " tensor(72),\n",
       " tensor(72),\n",
       " tensor(73),\n",
       " tensor(73),\n",
       " tensor(73),\n",
       " tensor(74),\n",
       " tensor(74),\n",
       " tensor(74),\n",
       " tensor(74),\n",
       " tensor(75),\n",
       " tensor(75),\n",
       " tensor(75),\n",
       " tensor(76),\n",
       " tensor(76),\n",
       " tensor(76),\n",
       " tensor(77),\n",
       " tensor(77),\n",
       " tensor(77),\n",
       " tensor(78),\n",
       " tensor(78),\n",
       " tensor(78),\n",
       " tensor(79),\n",
       " tensor(79),\n",
       " tensor(79),\n",
       " tensor(79),\n",
       " tensor(80),\n",
       " tensor(80),\n",
       " tensor(80),\n",
       " tensor(81),\n",
       " tensor(81),\n",
       " tensor(81),\n",
       " tensor(82),\n",
       " tensor(82),\n",
       " tensor(82),\n",
       " tensor(83),\n",
       " tensor(83),\n",
       " tensor(83),\n",
       " tensor(84),\n",
       " tensor(84),\n",
       " tensor(84),\n",
       " tensor(84),\n",
       " tensor(85),\n",
       " tensor(85),\n",
       " tensor(85),\n",
       " tensor(86),\n",
       " tensor(86),\n",
       " tensor(86),\n",
       " tensor(87),\n",
       " tensor(87),\n",
       " tensor(87),\n",
       " tensor(88),\n",
       " tensor(88),\n",
       " tensor(88),\n",
       " tensor(89),\n",
       " tensor(89),\n",
       " tensor(89),\n",
       " tensor(90),\n",
       " tensor(90),\n",
       " tensor(90),\n",
       " tensor(91),\n",
       " tensor(91),\n",
       " tensor(91),\n",
       " tensor(91),\n",
       " tensor(92),\n",
       " tensor(92),\n",
       " tensor(92),\n",
       " tensor(93),\n",
       " tensor(93),\n",
       " tensor(93),\n",
       " tensor(94),\n",
       " tensor(94),\n",
       " tensor(94),\n",
       " tensor(95),\n",
       " tensor(95),\n",
       " tensor(95),\n",
       " tensor(95),\n",
       " tensor(96),\n",
       " tensor(96),\n",
       " tensor(96),\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 33,\n",
       " 33,\n",
       " 33,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 35,\n",
       " 35,\n",
       " 35,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 37,\n",
       " 37,\n",
       " 37,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 41,\n",
       " 41,\n",
       " 41,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 43,\n",
       " 43,\n",
       " 43,\n",
       " 44,\n",
       " 44,\n",
       " 44,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 47,\n",
       " 47,\n",
       " 47,\n",
       " 48,\n",
       " 48,\n",
       " 48,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 51,\n",
       " 51,\n",
       " 51,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 54,\n",
       " 54,\n",
       " 54,\n",
       " 55,\n",
       " 55,\n",
       " 55,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 57,\n",
       " 57,\n",
       " 57,\n",
       " 57,\n",
       " 58,\n",
       " 58,\n",
       " 58,\n",
       " 59,\n",
       " 59,\n",
       " 59,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 63,\n",
       " 63,\n",
       " 63,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 65,\n",
       " 65,\n",
       " 65,\n",
       " 66,\n",
       " 66,\n",
       " 66,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 68,\n",
       " 68,\n",
       " 68,\n",
       " 69,\n",
       " 69,\n",
       " 69,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 71,\n",
       " 71,\n",
       " 71,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 73,\n",
       " 73,\n",
       " 73,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 75,\n",
       " 75,\n",
       " 75,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 77,\n",
       " 77,\n",
       " 77,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 79,\n",
       " 79,\n",
       " 79,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 82,\n",
       " 82,\n",
       " 82,\n",
       " 83,\n",
       " 83,\n",
       " 83,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 88,\n",
       " 88,\n",
       " 88,\n",
       " 89,\n",
       " 89,\n",
       " 89,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 91,\n",
       " 91,\n",
       " 91,\n",
       " 91,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 93,\n",
       " 93,\n",
       " 93,\n",
       " 94,\n",
       " 94,\n",
       " 94,\n",
       " 95,\n",
       " 95,\n",
       " 95,\n",
       " 96,\n",
       " 96,\n",
       " 96,\n",
       " 96,\n",
       " 97,\n",
       " 0.31,\n",
       " 0.63,\n",
       " 0.95,\n",
       " 1.27,\n",
       " 1.59,\n",
       " 1.91,\n",
       " 2.23,\n",
       " 2.54,\n",
       " 2.86,\n",
       " 3.18,\n",
       " 3.48,\n",
       " 3.79,\n",
       " 4.1,\n",
       " 4.41,\n",
       " 4.73,\n",
       " 5.04,\n",
       " 5.36,\n",
       " 5.68,\n",
       " 5.98,\n",
       " 6.27,\n",
       " 6.59,\n",
       " 6.89,\n",
       " 7.2,\n",
       " 7.51,\n",
       " 7.83,\n",
       " 8.13,\n",
       " 8.43,\n",
       " 8.75,\n",
       " 9.07,\n",
       " 9.35,\n",
       " 9.67,\n",
       " 9.98,\n",
       " 10.28,\n",
       " 10.6,\n",
       " 10.91,\n",
       " 11.22,\n",
       " 11.51,\n",
       " 11.83,\n",
       " 12.09,\n",
       " 12.38,\n",
       " 12.69,\n",
       " 12.98,\n",
       " 13.29,\n",
       " 13.59,\n",
       " 13.9,\n",
       " 14.2,\n",
       " 14.49,\n",
       " 14.78,\n",
       " 15.08,\n",
       " 15.39,\n",
       " 15.69,\n",
       " 16.01,\n",
       " 16.32,\n",
       " 16.62,\n",
       " 16.92,\n",
       " 17.23,\n",
       " 17.54,\n",
       " 17.85,\n",
       " 18.16,\n",
       " 18.47,\n",
       " 18.76,\n",
       " 19.06,\n",
       " 19.35,\n",
       " 19.63,\n",
       " 19.91,\n",
       " 20.2,\n",
       " 20.5,\n",
       " 20.81,\n",
       " 21.12,\n",
       " 21.44,\n",
       " 21.76,\n",
       " 22.05,\n",
       " 22.37,\n",
       " 22.69,\n",
       " 22.98,\n",
       " 23.27,\n",
       " 23.59,\n",
       " 23.89,\n",
       " 24.2,\n",
       " 24.52,\n",
       " 24.82,\n",
       " 25.11,\n",
       " 25.4,\n",
       " 25.72,\n",
       " 26.02,\n",
       " 26.32,\n",
       " 26.63,\n",
       " 26.94,\n",
       " 27.26,\n",
       " 27.57,\n",
       " 27.88,\n",
       " 28.18,\n",
       " 28.49,\n",
       " 28.8,\n",
       " 29.12,\n",
       " 29.44,\n",
       " 29.75,\n",
       " 30.06,\n",
       " 30.37,\n",
       " 30.68,\n",
       " 31.0,\n",
       " 31.32,\n",
       " 31.64,\n",
       " 31.96,\n",
       " 32.28,\n",
       " 32.59,\n",
       " 32.9,\n",
       " 33.21,\n",
       " 33.53,\n",
       " 33.84,\n",
       " 34.15,\n",
       " 34.44,\n",
       " 34.75,\n",
       " 35.07,\n",
       " 35.38,\n",
       " 35.69,\n",
       " 36.0,\n",
       " 36.29,\n",
       " 36.58,\n",
       " 36.88,\n",
       " 37.18,\n",
       " 37.49,\n",
       " 37.8,\n",
       " 38.09,\n",
       " 38.39,\n",
       " 38.69,\n",
       " 39.01,\n",
       " 39.29,\n",
       " 39.61,\n",
       " 39.92,\n",
       " 40.21,\n",
       " 40.51,\n",
       " 40.81,\n",
       " 41.12,\n",
       " 41.4,\n",
       " 41.71,\n",
       " 42.02,\n",
       " 42.34,\n",
       " 42.65,\n",
       " 42.97,\n",
       " 43.28,\n",
       " 43.59,\n",
       " 43.89,\n",
       " 44.2,\n",
       " 44.5,\n",
       " 44.82,\n",
       " 45.14,\n",
       " 45.46,\n",
       " 45.76,\n",
       " 46.08,\n",
       " 46.38,\n",
       " 46.69,\n",
       " 46.98,\n",
       " 47.29,\n",
       " 47.61,\n",
       " 47.92,\n",
       " 48.24,\n",
       " 48.56,\n",
       " 48.88,\n",
       " 49.2,\n",
       " 49.52,\n",
       " 49.84,\n",
       " 50.16,\n",
       " 50.47,\n",
       " 50.79,\n",
       " 51.11,\n",
       " 51.42,\n",
       " 51.74,\n",
       " 52.06,\n",
       " 52.38,\n",
       " 52.69,\n",
       " 53.01,\n",
       " 53.33,\n",
       " 53.65,\n",
       " 53.97,\n",
       " 54.27,\n",
       " 54.58,\n",
       " 54.9,\n",
       " 55.22,\n",
       " 55.51,\n",
       " 55.82,\n",
       " 56.14,\n",
       " 56.45,\n",
       " 56.76,\n",
       " 57.08,\n",
       " 57.38,\n",
       " 57.65,\n",
       " 57.94,\n",
       " 58.24,\n",
       " 58.53,\n",
       " 58.85,\n",
       " 59.17,\n",
       " 59.46,\n",
       " 59.78,\n",
       " 60.1,\n",
       " 60.42,\n",
       " 60.74,\n",
       " 61.06,\n",
       " 61.38,\n",
       " 61.7,\n",
       " 62.02,\n",
       " 62.34,\n",
       " 62.66,\n",
       " 62.98,\n",
       " 63.29,\n",
       " 63.58,\n",
       " 63.86,\n",
       " 64.17,\n",
       " 64.49,\n",
       " 64.81,\n",
       " 65.12,\n",
       " 65.41,\n",
       " 65.72,\n",
       " 66.04,\n",
       " 66.36,\n",
       " 66.68,\n",
       " 67.0,\n",
       " 67.32,\n",
       " 67.64,\n",
       " 67.96,\n",
       " 68.28,\n",
       " 68.59,\n",
       " 68.91,\n",
       " 69.23,\n",
       " 69.55,\n",
       " 69.86,\n",
       " 70.18,\n",
       " 70.5,\n",
       " 70.82,\n",
       " 71.13,\n",
       " 71.45,\n",
       " 71.77,\n",
       " 72.08,\n",
       " 72.4,\n",
       " 72.72,\n",
       " 73.04,\n",
       " 73.36,\n",
       " 73.68,\n",
       " 74.0,\n",
       " 74.32,\n",
       " 74.64,\n",
       " 74.96,\n",
       " 75.28,\n",
       " 75.6,\n",
       " 75.92,\n",
       " 76.23,\n",
       " 76.54,\n",
       " 76.85,\n",
       " 77.17,\n",
       " 77.49,\n",
       " 77.81,\n",
       " 78.12,\n",
       " 78.42,\n",
       " 78.74,\n",
       " 79.06,\n",
       " 79.38,\n",
       " 79.7,\n",
       " 80.01,\n",
       " 80.3,\n",
       " 80.6,\n",
       " 80.9,\n",
       " 81.22,\n",
       " 81.51,\n",
       " 81.83,\n",
       " 82.14,\n",
       " 82.46,\n",
       " 82.76,\n",
       " 83.08,\n",
       " 83.4,\n",
       " 83.72,\n",
       " 84.04,\n",
       " 84.36,\n",
       " 84.68,\n",
       " 85.0,\n",
       " 85.32,\n",
       " 85.64,\n",
       " 85.96,\n",
       " 86.28,\n",
       " 86.6,\n",
       " 86.92,\n",
       " 87.24,\n",
       " 87.53,\n",
       " 87.84,\n",
       " 88.15,\n",
       " 88.47,\n",
       " 88.79,\n",
       " 89.11,\n",
       " 89.43,\n",
       " 89.75,\n",
       " 90.07,\n",
       " 90.39,\n",
       " 90.71,\n",
       " 91.03,\n",
       " 91.35,\n",
       " 91.66,\n",
       " 91.98,\n",
       " 92.3,\n",
       " 92.62,\n",
       " 92.94,\n",
       " 93.25,\n",
       " 93.56,\n",
       " 93.87,\n",
       " 94.18,\n",
       " 94.49,\n",
       " 94.78,\n",
       " 95.07,\n",
       " 95.37,\n",
       " 95.68,\n",
       " 96.0,\n",
       " 96.31,\n",
       " 96.62,\n",
       " 96.93,\n",
       " 97.09]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct  =0\n",
    "    iterations = 0\n",
    "    iter_loss = 0\n",
    "    \n",
    "    test_loss = []\n",
    "    test_acuracy = []\n",
    "    \n",
    "    for i, (images,labels) in enumerate(test_load):\n",
    "        outputs = model2(images)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        loss += loss.item()\n",
    "        _,predict = torch.max(outputs,dim=1)\n",
    "        correct += (predict == labels).sum().item()\n",
    "        iterations += 1\n",
    "        \n",
    "        test_loss.append(loss/iterations)\n",
    "        test_accuracy.append(100 * correct / len(test_dataset))\n",
    "    \n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7870ed30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearNet(\n",
       "  (linear1): Linear(in_features=784, out_features=1200, bias=False)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear2): Linear(in_features=1200, out_features=1200, bias=False)\n",
       "  (linear3): Linear(in_features=1200, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_path = './small_linear_model/'\n",
    "load_path = load_path + 'model.pth.tar'\n",
    "\n",
    "model.load_state_dict(torch.load(load_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0d5ec8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(scores,targets,T=5):\n",
    "    soft_pred = nn.Softmax(scores / 5)\n",
    "    soft_targets = nn.Softmax(targets / 5)\n",
    "    loss = nn.MSELoss(soft_pred,soft_targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8486651f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'MSELoss' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-26c44da9f4b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    770\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 772\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'MSELoss' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "#Distrill\n",
    "model3 = SmallLinearNet()\n",
    "optimizer = torch.optim.Adam(model3.parameters(),lr=5e-3)\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "epochs = 5\n",
    "temp = 5 \n",
    "iterations = 0\n",
    "correct = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i ,(images,labels) in enumerate(train_load):\n",
    "        scores = model2(images)\n",
    "        targets = model(images)\n",
    "        loss = my_loss(scores,targets,T=temp)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        correct += (scores == labels).sum()\n",
    "        iterations += 1\n",
    "\n",
    "train_accuracy.append(100 * correct / len(train_load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac9c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
